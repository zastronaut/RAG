[1 tool called]

下面是一份可直接照做的评估使用说明（不改代码即可运行）。

一、评估脚本位置与作用
- 文件：backend/eval_lite.py
- 能力：
  - 从现有向量库自动构造“完形填空”评测集（cloze）
  - 评测指标：Retrieval Hit@K、生成字符级F1、语义相似度（嵌入余弦）、延迟P50/P95
  - 可选：答案句子与证据句子的语义覆盖率（coverage）
- 运行方式：推荐用模块方式
  - python -m backend.eval_lite ...

二、前置条件
1) 已有向量库
- 先通过前端上传文档完成入库（向量落在 backend/data/vectors）
- 如果更换了嵌入模型（如切到 BAAI/bge-m3），必须删除 backend/data/vectors 后重建向量库

2) 本地 LLM 就绪（用于生成答案）
- 启动 Ollama 服务：ollama serve
- 确保已拉取并设置好模型（env OLLAMA_MODEL），默认 qwen3:1.7b
- 评测时建议温度为 0：设置环境变量 EVAL_MODE=1（脚本会把温度设为0，保证可复现）

三、快速开始
A. 仅构建评测集
- Linux/Mac:
  python -m backend.eval_lite --build 150 --out eval_set_lite.jsonl
- Windows CMD:
  python -m backend.eval_lite --build 150 --out eval_set_lite.jsonl

B. 运行评测（推荐先设温度为0）
- Linux/Mac:
  EVAL_MODE=1 python -m backend.eval_lite --run --file eval_set_lite.jsonl --k 5 --coverage --details details.jsonl
- Windows CMD:
  set EVAL_MODE=1 && python -m backend.eval_lite --run --file eval_set_lite.jsonl --k 5 --coverage --details details.jsonl
- Windows PowerShell:
  $env:EVAL_MODE=1; python -m backend.eval_lite --run --file eval_set_lite.jsonl --k 5 --coverage --details details.jsonl

C. 一条命令完成“构建+评测”
- Linux/Mac:
  EVAL_MODE=1 python -m backend.eval_lite --build 150 --out eval_set_lite.jsonl --run --file eval_set_lite.jsonl --k 5 --coverage --details details.jsonl
- Windows CMD:
  set EVAL_MODE=1 && python -m backend.eval_lite --build 150 --out eval_set_lite.jsonl --run --file eval_set_lite.jsonl --k 5 --coverage --details details.jsonl

四、参数说明
- --build N：从现有向量库抽样构造 N 条完形填空样本
- --out FILE：评测集输出路径（jsonl）
- --run：根据 --file 运行评测
- --file FILE：评测集路径（jsonl）
- --k：检索与评测时的 Top-K（默认5）
- --coverage：启用证据覆盖率与答案-证据平均语义相似度
- --coverage_thresh：覆盖率阈值（默认0.6）
- --details FILE：保存每条样本的详细评测记录（jsonl）

五、输出结果
- 终端会打印总结报告（JSON），示例：
  {
    "N": 150,
    "retrieval_Hit@5": 0.68,
    "gen_F1": 0.41,
    "gen_semantic_sim": 0.72,
    "latency_p50_s": 1.23,
    "latency_p95_s": 2.87,
    "coverage": 0.54,
    "max_sim_avg": 0.66
  }
- 若指定 --details details.jsonl，会生成逐条记录，包含：
  - question、answer_gold、answer_pred、answer_pred_raw
  - pred_sources（检索到的来源与页码）
  - retrieval_hit、f1、semantic_sim
  - 可选：coverage、max_sim_avg
  - latency_s

六、可调节项与实验开关
- 切分参数（影响召回/延迟）
  - CHUNK_SIZE（默认500）、CHUNK_OVERLAP（默认100）
- 检索策略
  - RETRIEVAL_METHOD=similarity 或 mmr（注意：当前评估脚本在检索命中计算处直接用 similarity_search；如果要严格对比 MMR 的 Hit@K，需要我帮你把 eval_lite 的检索调用改成使用 get_retrieved_docs）
  - RETRIEVAL_FETCH_K（MMR 预取量，默认 max(20, K*4)）
- 生成稳定性
  - EVAL_MODE=1 或设置 LLM_TEMPERATURE=0
- 嵌入模型
  - EMBED_MODEL（例如 BAAI/bge-m3）
  - 更换后需删除 backend/data/vectors 并重建

七、常见问题排查
- RuntimeError: Vector store is empty
  - 先通过前端上传文档入库，再运行评测
- 模块导入错误（ModuleNotFoundError）
  - 用模块方式运行：python -m backend.eval_lite
- 评测很慢
  - LLM 推理与向量相似度计算会耗时；可适当减小 --build 数量或 --k
- 嵌入维度不匹配/检索报错
  - 切换 EMBED_MODEL 后删除 backend/data/vectors 目录并重建
- Ollama 连接失败
  - 先执行 ollama serve；确认 OLLAMA_HOST（默认 http://localhost:11434）与 OLLAMA_MODEL

八、建议的基线流程
1) 通过前端上传你的语料并完成向量化
2) 构建 150 条评测集
3) 开启评测（EVAL_MODE=1，K=5，带 coverage）
4) 记录一版“基线”指标
5) 做检索/切分/模型的改动，再重复 2–4，写入 README 的“优化前后对比”

下面我把你 RAG Lite 评估中出现的所有指标 **逐个解释清楚**，包括它们代表什么、为什么重要、正常范围是多少、以及如何解读结果（含你当前的结果）。

---

# 📊 **RAG Lite 评估指标详解（你的 eval_lite.py）**

## 1️⃣ **N（样本数量）**

```
"N": 150
```

评估集的样本数量，即你构造的 cloze 测试数据条数。

一般建议：

* > =100 才有统计意义
* 越多越稳定（200~500）

---

# 🔍 **Retrieval Phase（检索阶段指标）**

## 2️⃣ **retrieval_Hit@5**

```
"retrieval_Hit@5": 0.54
```

**定义：模型检索的前 5 个文档中，是否包含真正来源文档。**

Hit@5 = 54% 说明：

* 有 **54% 的问题** 检索到了正确资料（但可能不是第一名）。
* 检索 recall 的核心指标。

### 提升 Hit@5 的手段：

* 更好的 embedding（例如 bge-m3 → bge-large → bge-m3-hybrid）
* 更好的 chunking（小 chunk）
* better retriever（hybrid、bm25+dense）
* reranker（重排）

---

# ✏️ **Generation Phase（生成阶段指标）**

## 3️⃣ **gen_F1（生成 F1，字符级）**

```
"gen_F1": 0.1537
```

### 定义：

* 将模型生成答案与真实答案做字符级 F1 。
* 数字类答案特别敏感：年份、百分比、数字直接 mismatch → F1=0

### 对你的系统含义：

* 你这是 **mask 填空评估**（句子中挖一个词/数）
* 小模型（1.7B）自然 F1 不会特别高
* > 0.15 已经属于“能填对一部分信息”

### 常见范围：

* 小 LLM：0.05–0.20
* 中型 LLM（7B）：0.20–0.40
* GPT4 级：0.50+

---

## 4️⃣ **gen_semantic_sim（语义相似度）**

```
"gen_semantic_sim": 0.538
```

### 定义

生成答案与正确答案的语义向量相似度（embedding cosine）。

### 好处

* 哪怕 F1 不高（字不一样），但语义有可能接近（如“北京大学” vs “北大”）。

### 正常范围

* 0.3–0.4 → 很差（随便答）
* 0.5–0.6 → 合理（方向正确但细节不对）
* 0.7+ → 较好

你的 0.538 表示：

> 大部分答案方向是合理的，但细节缺失或不准确。

---

# ⚡ **Latency Phase（响应速度）**

## 5️⃣ **latency_p50_s（中位数延迟）**

```
"latency_p50_s": 6.2s
```

* 从检索开始 → LLM 生成答案结束 的总耗时。
* p50: 一半问题在此时间内完成。

6 秒属于 **Ollama 小模型的正常水平**。

---

## 6️⃣ **latency_p95_s（95 百分位延迟）**

```
"latency_p95_s": 12.8s
```

最慢的 5% 查询需要 12 秒。
通常因为：

* 检索 hit 不好，需要复杂生成
* 某些 chunk 太长
* embedding 速度不稳定

---

# 🧠 **Coverage Phase（证据覆盖率）**

你启用了 `--coverage`，所以有额外两个指标：

---

## 7️⃣ **coverage（证据覆盖率）**

```
"coverage": 0.4313
```

### 定义：

* 将模型的 **生成答案** 切成句子
* 对每一句在检索到的文档中找到最相似的句子（embedding）
* 相似度 ≥ threshold（0.6）则算“找到证据”

Coverage = 覆盖句子数 / 总句子数

### 解读：

43% 意味着：

> 模型生成的内容，有 43% 可以在检索文档中找到依据。

太高（>0.7）表示：

> 模型只是复读 chunk（几乎没有总结能力）

太低（<0.3）表示：

> 模型经常胡编（hallucinate）

你的 0.43 属于 **健康区间**：不是纯粹复读，也没有太多幻觉。

---

## 8️⃣ **max_sim_avg（最大相似度平均值）**

```
"max_sim_avg": 0.5759
```

### 定义：

* 每个生成句子与文档句子匹配的 **最高相似度**
* 然后对所有句子取平均

### 解读：

* ~0.5–0.6：半相关，有些句子能找到依据
* ~0.7+：很强依据（说明模型 strongly grounded）

你的 0.575 表示：

> 模型生成的内容大部分“接近”资料来源，但不总是精确依赖它。

---

# 📌 **最终指标解读（你的结果）**

| 指标           | 你当前       | 说明               |
| ------------ | --------- | ---------------- |
| Hit@5        | **0.54**  | 检索命中率不错，但可更高     |
| F1           | **0.153** | 能填对基本关键词，小模型正常水平 |
| Semantic Sim | **0.538** | 大部分答案方向正确但不准确    |
| p50 Latency  | **6.28s** | 小模型正常速度          |
| Coverage     | **0.43**  | 模型生成有一半有依据       |
| Max Sim Avg  | **0.57**  | 生成内容和来源有中等关联     |

整体来说：

> **你已经从一个“不可用的 RAG” → “可用的 RAG”**
> 检索和生成已经达到能投入场景测试的水准。

---

# 需要我帮你把这些指标整理成 **面试官可读的一段总结** 吗？

或者你想让我根据这些指标推荐一套 **进一步优化方案**？
